{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Project\n",
    "\n",
    "## Submitted by: Hardik Garg as part of DS+ML course offered by CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn import model_selection\n",
    "import os, re\n",
    "import time, operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-1: Data Preprocessing\n",
    "\n",
    "Here we organize the data into the traditional x,y and train,test arrays so that they can be fed to our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop_words list-1\n",
    "## note: this one is very long and program will take too long to run hence this is not used\n",
    "\n",
    "stop_words = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop_words list-2 \n",
    "## note: this one is shorter and less accurate but runs fast. This is used\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## storing documents in x,y\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "## accessing directories and thus documents\n",
    "for category in os.listdir(\"/home/hardeekh/Desktop/sublime/data science/Machine Learning/naive_bayes_project/20_newsgroups\"):\n",
    "    \n",
    "    for document in os.listdir(\"/home/hardeekh/Desktop/sublime/data science/Machine Learning/naive_bayes_project/20_newsgroups/\"+category):\n",
    "        \n",
    "        with open(\"/home/hardeekh/Desktop/sublime/data science/Machine Learning/naive_bayes_project/20_newsgroups/\"+category+\"/\"+document, \"r\", encoding=\"ISO-8859-1\") as file_obj:\n",
    "            \n",
    "            x.append([document, file_obj.read().lower()])\n",
    "            y.append(category)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doing a 3:1 train_test_split\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The block executed in: 9.69997239112854 seconds\n",
      "total features / words: 106407\n"
     ]
    }
   ],
   "source": [
    "## creating word-freq dictionary, takes around 10 seconds to execute\n",
    "\n",
    "## time module for calculating time\n",
    "a=time.time()\n",
    "\n",
    "## creating frequency dictionary\n",
    "word_dict = {}\n",
    "\n",
    "for string in x_train:\n",
    "    \n",
    "    ## using regex to extract individual words out of a long news article.\n",
    "    words = re.findall(\"[a-z]+\", string[1])\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        ## adding the word in dictionary if it is not a stop word and has length > 2\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            \n",
    "            if word in word_dict:\n",
    "                word_dict[word]+=1\n",
    "                \n",
    "            else:\n",
    "                word_dict[word]=1\n",
    "                \n",
    "## sort the dictionary in descending order of frequency\n",
    "sorted_word_dict = sorted(word_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "## sort frequency values\n",
    "\n",
    "## array having only values\n",
    "value_arr = [sorted_word_dict[i][1] for i in range(len(sorted_word_dict))]\n",
    "\n",
    "## array having only keys. This serves as our vocabulary\n",
    "vocab = [sorted_word_dict[i][0] for i in range(len(sorted_word_dict))]\n",
    "\n",
    "print(\"The block executed in:\",time.time()-a,\"seconds\")\n",
    "print(\"total features / words:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApEElEQVR4nO3deZxcdZnv8c9TW1fvnZ2QBBIhgICKMbIoVxFlcwFG0cHxCnq5w9yRcXR0HGFGxX3GmXEZxzsqKnfQmRERFxAZMbK5sYUtECAkQIAsZE+603tVPfeP86tOJaa7K0mdU92V7/v1qled8zvbU92VPP1bzu+YuyMiIlJLqXoHICIijUfJRUREak7JRUREak7JRUREak7JRUREak7JRUREai7W5GJmq83sETN7yMyWhrKpZrbEzFaG9ymh3Mzsq2a2ysyWmdmiivNcHPZfaWYXxxmziIgcuCRqLq9z9xPcfXFYvxy41d0XAreGdYBzgIXhdSnwdYiSEXAlcBJwInBlOSGJiMjEVI9msfOAa8LyNcD5FeXf9cjdQJeZzQbOApa4+1Z33wYsAc5OOGYREdkHmZjP78AvzcyBb7r7VcAsd18ftr8AzArLc4DnK45dE8pGK9+NmV1KVOOhtbX1FUcedTSPr+9mTlczU1tztfxMIiIN6f7779/s7jNqca64k8up7r7WzGYCS8zsicqN7u4h8RywkLiuAli8eLH/4o7fsfizv+KT5x3HRafMr8UlREQampk9W6tzxdos5u5rw/tG4CdEfSYbQnMX4X1j2H0tMK/i8LmhbLTyMaXNACiVNHeaiEjSYksuZtZqZu3lZeBM4FHgRqA84uti4IawfCNwURg1djKwIzSf3QKcaWZTQkf+maFsTKmQXIrKLSIiiYuzWWwW8BOL/pPPAP/l7r8ws/uA68zsEuBZ4B1h/5uBNwKrgD7gvQDuvtXMPgPcF/b7tLtvHe/iqZA2VXMREUlebMnF3Z8GXraX8i3A6/dS7sBlo5zrauDqfbl+OhWaxfRIARGRxDXsHfq7msWUXEREktbwyUXNYiIiyWvY5LKrWazOgYiIHIQaNrmE3EJR2UVEJHENm1zMDDN16IuI1EPDJheIbqRUchERSV5DJ5eUGcVSvaMQETn4NHRySaeMYknZRUQkaQ2dXFpyaXqHivUOQ0TkoNPQyaU9n2HnQKHeYYiIHHQaOrm05TPsHFRyERFJWkMnl6ZMmsGCmsVERJLW0Mkll04xVFCHvohI0ho7uWSUXERE6qHhk8ugkouISOIaPrmo5iIikryGTi5NadVcRETqoaGTSzqlucVEROpByUVERGquoZOLaeJKEZG6aOjkkk6Bq+YiIpK4hk4uKTOKSi4iIolr+ORS0mOORUQS1/jJRblFRCRxDZ1c0ik0WkxEpA4aOrlEjzlWchERSVpjJ5eUoYqLiEjyGju5GBotJiJSBw2eXHSHvohIPTR8cnHXjZQiIklr+OQCaDiyiEjCGjq5pMOn04gxEZFkNXRysZGai5KLiEiSGjq5pFNKLiIi9dDQySWfiT7ewLDm3RcRSVLsycXM0mb2oJndFNYXmNk9ZrbKzH5gZrlQ3hTWV4Xt8yvOcUUoX2FmZ1V77Y7mLAA9A8O1/VAiIjKmJGouHwAer1j/AvBldz8S2AZcEsovAbaF8i+H/TCzY4ELgeOAs4F/M7N0NRfOZ6PdVHMREUlWrMnFzOYCbwK+HdYNOB24PuxyDXB+WD4vrBO2vz7sfx5wrbsPuvszwCrgxGqun89GH69/uHigH0VERPZB3DWXrwB/A5SrDtOA7e5eCOtrgDlheQ7wPEDYviPsP1K+l2NGmNmlZrbUzJZu2rQJgHymXHNRchERSVJsycXM3gxsdPf747pGJXe/yt0Xu/viGTNmAJDPKbmIiNRDJsZzvxo418zeCOSBDuBfgC4zy4TayVxgbdh/LTAPWGNmGaAT2FJRXlZ5zJhUcxERqY/Yai7ufoW7z3X3+UQd8re5+7uA24ELwm4XAzeE5RvDOmH7bR5NCnYjcGEYTbYAWAjcW00M5T4XdeiLiCQrzprLaD4KXGtmnwUeBL4Tyr8DfM/MVgFbiRIS7r7czK4DHgMKwGXuXlVVZNdoMdVcRESSlEhycfc7gDvC8tPsZbSXuw8Abx/l+M8Bn9vX65aTi0aLiYgkq6Hv0G9WchERqYuGTi75bIps2ujuL4y/s4iI1ExDJxczo7M5y45+Tf8iIpKkhk4uEM0v1q25xUREEjVucjGzt5tZe1j+mJn92MwWxR9abTRl0gwVNBRZRCRJ1dRcPu7uPWZ2KvAGoiHDX483rNrJpY3hopKLiEiSqkku5aFWbwKucvefA7n4QqqtTDpFoaiHhYmIJKma5LLWzL4J/DFws5k1VXnchJBNG0OquYiIJKqaJPEO4BbgLHffDkwFPhJnULWUTacoKLmIiCRq3OTi7n3ARuDUUFQAVsYZVC1lUsawmsVERBJVzWixK4nmA7siFGWB/4gzqFrKplPq0BcRSVg1zWJ/BJwL9AK4+zqgPc6gaknJRUQkedUkl6Ew9b0DmFlrvCHVVjZtFEpqFhMRSVI1yeW6MFqsy8z+FPgV8K14w6odDUUWEUneuFPuu/s/m9kZQDdwNPAJd18Se2Q1oqHIIiLJGze5hKc//qacUMys2czmu/vquIOrBQ1FFhFJXjXNYj8EKv93LoaySSGTSmkosohIwqpJLhl3HyqvhOVJM/1LNqO5xUREklZNctlkZueWV8zsPGBzfCHVVjalocgiIkkbt88F+D/Af5rZ1wADngcuijWqGsqkjZJDseSkU1bvcEREDgrVjBZ7CjjZzNrC+s7Yo6qhbDqqnA0XS6RT6TpHIyJycKhmtFgT8DZgPpAxi/76d/dPxxpZjeRCchkqlshnlVxERJJQTbPYDcAO4H5gMN5waq8tH33E3sECHflsnaMRETk4VJNc5rr72bFHEpO2pugj9gwUmN1Z52BERA4S1YwW+72ZvST2SGJS7sRfu72/zpGIiBw8qqm5nAq8x8yeIWoWM8Dd/aWxRlYjszvz0YLuoxQRSUw1yeWc2KOIUbkTf7BQrHMkIiIHj2qeRPksMA84PSz3VXPcRJHLRKEOFnQjpYhIUhr+SZRNSi4iIolr+CdRNmXKzWJKLiIiSWn4J1GONIsNq89FRCQpDf8kSjWLiYgkb8zRYhbN9fID4Bgm6ZMomzIpzGBANRcRkcSMmVzc3c3sZnd/CTBpEkolM6M5m1ZyERFJUDXNYg+Y2Sv39cRmljeze83sYTNbbmafCuULzOweM1tlZj8ws1wobwrrq8L2+RXnuiKUrzCzs/Y1luZsmn4lFxGRxFSTXE4C7jKzp8xsmZk9YmbLqjhukOjemJcBJwBnm9nJwBeAL7v7kcA24JKw/yXAtlD+5bAfZnYscCFwHHA28G9mtk/TG+ezafqGlFxERJJSzR36+1xTgKhJDSg/+yUbXg6cDvxJKL8G+CTwdeC8sAxwPfC10OdzHnCtuw8Cz5jZKuBE4K5qY2nKpBhSh76ISGKqqbn4KK9xmVnazB4CNhL12TwFbHf3QthlDTAnLM8hesolYfsOYFpl+V6OqbzWpWa21MyWbtq0abdtOSUXEZFEVVNz+TlRMjEgDywAVhA1U43J3YvACWbWBfyEaNRZLNz9KuAqgMWLF++W/HKZFENFJRcRkaRU85jj3abbN7NFwPv25SLuvt3MbgdOIbpfJhNqJ3OBtWG3tURzmK0xswzQCWypKC+rPKYqubRqLiIiSdrnCSjd/QGiTv4xmdmMUGPBzJqBM4DHgduBC8JuFxM96RLgxrBO2H5b6Le5EbgwjCZbACwE7t2XmNUsJiKSrHFrLmb2oYrVFLAIWFfFuWcD14SRXSngOne/ycweA641s88CDwLfCft/B/he6LDfSjRCDHdfbmbXAY8BBeCy0NxWtaZMip2DhfF3FBGRmqimz6VyksoCUR/Mj8Y7yN2XAS/fS/nTRKO99iwfAN4+yrk+B3yuilj3SjUXEZFkVdPn8qkkAolTXnfoi4gkqprnuSwp952E9SlmdkusUdVYW1NGzWIiIgmqpkN/hrtvL6+4+zZgZmwRxaAtn6F7QMlFRCQp1SSXopkdVl4xs8Op8ibKiaIlm2GoUKJYmlRhi4hMWtV06P8d8Fszu5PoRsr/AVwaa1Q11pyLcmj/cJG2pmo+soiIHIhqOvR/EW6cPDkUfdDdN8cbVm0156KP2T+k5CIikoRqOvT/CBh295vc/SagYGbnxx5ZDTVno0mU+zUzsohIIqrpc7nS3XeUV0Ln/pWxRRSDllxILhqOLCKSiGqSy972mVRtS+WaS9+QRoyJiCShmuSy1My+ZGZHhNeXgPvjDqyWmlVzERFJVDXJ5f3AEPCD8BoELoszqFpTn4uISLKqGS3WGyaZ/Ky77xxv/4lIfS4iIskas+ZiZu8zs+eAZ4FnzexZM9unZ7lMBPmRPhclFxGRJIyaXMzsY8CbgdPcfZq7TwNeB5wTtk0a5ZqLJq8UEUnGWDWXdwNvDVPkAyPT5b8DuCjuwGqp3KGvmouISDLGSi4enrGyZ2E/MKkejpLPqENfRCRJYyWXtWb2+j0Lzex0YH18IdVeKmXksyl16IuIJGSs0WJ/CdxgZr9l130ti4FXA+fFHVitNWfTuolSRCQho9Zc3H05cDzwa2B+eP0aOD5sm1RmtDexoXuw3mGIiBwUxrzPJfS5XJ1QLLHqyGfp1dMoRUQSUc0d+g2hpSlDrzr0RUQScdAkl9Zcmj7VXEREEjHWTZS3hvcvJBdOfFpyGTWLiYgkZKw+l9lm9irgXDO7lugRxyPc/YFYI6ux6W05Nu8cwt0xs/EPEBGR/TZWcvkE8HFgLvClPbY5cHpcQcVhelsTQ8USPYMFOvLZeocjItLQRk0u7n49cL2ZfdzdP5NgTLHobIkSyo6+YSUXEZGYVTPl/mfM7FzgNaHoDne/Kd6waq+rOSSX/mHm1TkWEZFGN+5oMTP7e+ADwGPh9QEz+3zcgdVaV0sOgO19w3WORESk8Y1bcwHeBJzg7iUAM7sGeBD42zgDq7Wu0Cy2pVd36YuIxK3a+1y6KpY7Y4gjdodPayGdMlZumJQP0xQRmVSqqbn8PfCgmd1ONBz5NcDlsUYVg6ZMmq7mLFv7huodiohIw6umQ//7ZnYH8MpQ9FF3fyHWqGLS2ZJlR7/6XERE4lZNzQV3Xw/cGHMssetszrJDHfoiIrE7aOYWA5jakmNLr5rFRETidlAll5kdeTZ2/8GTm0VEpMbGTC5mljazJ/bnxGY2z8xuN7PHzGy5mX0glE81syVmtjK8TwnlZmZfNbNVZrbMzBZVnOvisP9KM7t4f+IBmNXRxJbeIYYKpf09hYiIVGHM5OLuRWCFmR22H+cuAB9292OBk4HLzOxYopFmt7r7QuBWdo08OwdYGF6XAl+HKBkBVwInAScCV5YT0r46pCMPwKadutdFRCRO1TSLTQGWm9mtZnZj+TXeQe6+vjxzsrv3AI8Dc4DzgGvCbtcA54fl84DveuRuoMvMZgNnAUvcfau7bwOWAGdX/xF3mRWSywY1jYmIxKqa0WIfP9CLmNl84OXAPcCsMPoM4AVgVlieAzxfcdiaUDZa+Z7XuJSoxsNhh+29ojWzowmADTuUXERE4jRuzcXd7wRWA9mwfB9Q9bNczKwN+BHwQXfv3uPcTjR9/wFz96vcfbG7L54xY8Ze9zlENRcRkURUM3HlnwLXA98MRXOAn1ZzcjPLEiWW/3T3H4fiDaG5i/C+MZSvhd0mLJ4bykYr32dTWnJk08aGHvW5iIjEqZo+l8uAVwPdAO6+Epg53kEWPe7xO8Dj7l75sLEbgfKIr4uBGyrKLwqjxk4GdoTms1uAM81sSujIPzOU7bNUypjZnlezmIhIzKrpcxl096Hyo4HNLEN1TVmvBt4NPGJmD4WyvwX+AbjOzC4BngXeEbbdDLwRWAX0Ae8FcPetZvYZouY4gE+7+9Yqrr9Xszqa2NCj5CIiEqdqksudZva3QLOZnQG8D/jZeAe5+2+JJrrcm9fvZX8nqiXt7VxXA1dXEeu4ZnXkWblRMyOLiMSpmmaxy4FNwCPAnxHVMD4WZ1BxmtWRV4e+iEjMqpkVuRQeEHYPUXPYilDLmJRmdeTpGSjQN1SgJVfVvJ0iIrKPqhkt9ibgKeCrwNeAVWZ2TtyBxWVW+V6Xbo0YExGJSzV/un8ReJ27rwIwsyOAnwP/HWdgcam8S3/B9NY6RyMi0piq6XPpKSeW4GmgJ6Z4YqcpYERE4jdqzcXM3hoWl5rZzcB1RH0ub2fXsOBJp9wstm67kouISFzGqrm8JbzywAbgtcBpRCPHmmOPLCbt+SwvmtHKXU9vqXcoIiINa9Sai7u/N8lAknTSgmnctGwdpZKTSo12K46IiOyvakaLLTCzL5nZj/dlyv2J7OWHddEzUODpzbqZUkQkDtWMFvsp0RxhPwMa4hGOiw6LnjW2dPU2jpzZXudoREQaTzXJZcDdvxp7JAk6YkYr7fkMy9bu4MJ6ByMi0oCqSS7/YmZXAr8ERu48LD9lcjIyM158SAcPPbe93qGIiDSkapLLS4hmNz6dXc1iHtYnrRfPbuf6+9dQLDlpdeqLiNRUNcnl7cCL3H0o7mCS9NK5XVxz17M8vr6b4+d01jscEZGGUs0d+o8CXTHHkbiXzI0SytObe+sciYhI46mm5tIFPGFm97F7n8u5cQWVhLlTovtAn9/aV+dIREQaTzXJ5crYo6iDllyGaa05ntqke11ERGqtmue53JlEIPVw4oKp3LRsPZ9487F0teTqHY6ISMOo5g79HjPrDq8BMyuaWXcSwcXtj185j6FCiQc1JFlEpKbGTS7u3u7uHe7eQTRh5duAf4s9sgS8cv5U0injbk1iKSJSU9WMFhvhkZ8CZ8UTTrJamzK8ZE4ndz+ztd6hiIg0lHH7XCqe6wJRMloMNMzDUF595DS+eefT9A4WaG2qZnyDiIiMp5qay1sqXmcRPYXyvDiDStKrjphOoeT86vEN9Q5FRKRhVDNarGGf6wJwyoumMa01x+1PbOS8E+bUOxwRkYYw1mOOPzHGce7un4khnsSlUsZrj5rBHU9u0jxjIiI1MlazWO9eXgCXAB+NOa5EnXbMTLb2DvHwmu31DkVEpCGM9ZjjL5aXzawd+ADwXuBa4IujHTcZvWbhdNIp4+Zl60ceJCYiIvtvzA59M5tqZp8FlhElokXu/lF335hIdAnpasnx6iOn87Nl6yiVvN7hiIhMeqMmFzP7J+A+otFhL3H3T7r7tsQiS9g7Fs9lQ/cg1973fL1DERGZ9MaquXwYOBT4GLCuYgqYnkaZ/qXSmcceQsrgRw+sqXcoIiKT3qjJxd1T7t5cOf1LeLWHqWAaSi6T4h2L53H/s9tYs03T8IuIHIh9mv6l0b37lMMB+PWTm+sciYjI5KbkUuHFh3Qwf1oL371rdb1DERGZ1JRcKqRSxrtPmc8TL/SwamNPvcMREZm0YksuZna1mW00s0cryqaa2RIzWxnep4RyM7OvmtkqM1tmZosqjrk47L/SzC6OK96yt7x0NmZw07L1cV9KRKRhxVlz+Xfg7D3KLgdudfeFwK1hHeAcYGF4XQp8HaJkRPSY5ZOAE4ErywkpLjM78pzyomlc/dtn6BkYjvNSIiINK7bk4u6/BvZ8UMp5wDVh+Rrg/Iry74bnxdwNdJnZbKJZmJe4+9Zwj80S/jBh1dxfvO5IugcK/OTBtXFfSkSkISXd5zLL3cvtTS8As8LyHKDy7sU1oWy08j9gZpea2VIzW7pp06YDCvKUI6axcGYbP9ANlSIi+6VuHfru7kDN5lpx96vcfbG7L54xY8YBncvMeM1RM1i+rptH1+6oUYQiIgePpJPLhtDcRXgvz1G2FphXsd/cUDZaeewuOXUBAJ/9+WOab0xEZB8lnVxuBMojvi4GbqgovyiMGjsZ2BGaz24BzjSzKaEj/8xQFrtDu5q54pxjuPvprTyi2ouIyD6Jcyjy94G7gKPNbI2ZXQL8A3CGma0E3hDWAW4GngZWAd8C3gfg7luBzxBNoHkf8OlQloh3LJ5HLpPiX29bmdQlRUQawriPOd5f7v7OUTa9fi/7OnDZKOe5Gri6hqFVbUprjg++YSH/+IsVPPT8dk6Y11WPMEREJh3doT+Oi06ZT3tThi/+cgVRDhQRkfEouYyjrSnDX51xFL9ZuVn3vYiIVEnJpQrvedV8Xjavi8/f/Dhbdg7WOxwRkQlPyaUKqZTxD299Cd39Bd717XvY2D1Q75BERCY0JZcqvXh2B1dd9AqeeKGHD//wYd37IiIyBiWXfXDa0TP52JtezG9Wbub933+QgeFivUMSEZmQYhuK3KguOXUBJXc+f/MTrNjQw7cuWsyC6a31DktEZEJRzWUfmRmXvuYI/v29r2TNtj7e/o3f8/j67nqHJSIyoSi57KfTjp7JtZeeQrHkXPD133PL8hfqHZKIyISh5HIATpjXxS8++BoOn9bKn33vfj7yw4dZu72/3mGJiNSdkssBmtWR5yeXvYr3vGo+Nzy8jnO+8mvuempLvcMSEakrJZcaaMqk+eS5x3HzX/4P2poy/Mm37+bq3z6j6WJE5KCl5FJDR85sY8mHXsupR07n0zc9xkVX38vTm3bWOywRkcQpudRYa1OGa957IleccwwPPreds7/yG77wiyd4dktvvUMTEUmMNWLTzeLFi33p0qX1DoN12/v55I3L+eVjG0gZnH7MLC4/52iOnNle79BERP6Amd3v7otrcS7dRBmjQ7uaueqixTy/tY//uvc5rv7tM/z6yU38zdlH8z9PPpx8Nl3vEEVEYqGaS4I29gzw4ese5jcrNzN3SjN/9YajOPeEQ8mm1TopIvVXy5qL/ldL0Mz2PN+75CT+45KTaM6m+fAPH+a0f7qDa+99ju6B4XqHJyJSM6q51Emp5Nz6xEa+8qsnWb6um1w6xRnHzeL1x8zkrOMOobVJLZYikiz1uTSAVMo449hZnH7MTO5bvZWbH1nPzY+8wM+Xraez+THetmgu57/8UI4/tJNUyuodrojIPlHNZQIplZz7Vm/lW7+JOv6HiiXa8xlOWjCV1x49k7OOncXMjny9wxSRBlXLmouSywS1tXeIWx/fwAPPbefOFRtZtyN6+uXCmW2cdvQMFs+fyrGzO5g3taXOkYpIo1ByGUcjJJdK7s6TG3Zy6xMb+N2qzdz7zFaGi9Hv7YgZrZy4YCovndvFKw6fwsKZbZipGU1E9p2SyzgaLbnsqXewwMqNO7n/2W3c+eQmHnx2Gz2DBQAO7cyz6PApLD58CicfMY2jZrarz0ZEqqLkMo5GTy57KpacNdv6+M3Kzdz11BYeeG4b60MzWksuzbGzOzh14XSOO7STI2a0Mm9qi+6tEZE/oOQyjoMtuezNc1v6uHf1Vh5du4Olz25l+bpuyr/qlMHCme0cM7udow9p5/CprRw1q415U1s0a4DIQUxDkWVch01r4bBpLVzwirkA9AwMs3LjTlZt2MlzW/tYvm4Hv39qCzc8tG7kGDOY3ZHnqEPaOXpWO3OntvCyuZ28eHaHajoisk+UXA4S7fksiw6bwqLDpuxWvnOwwOrNvazc2MPqzX08u6WXx9f38LtVm0cGDaRTxsz2JmZ35pk3tYXZnc1Mb8txSGeeqa05prc1MasjT2dzth4fTUQmICWXg1xbU4bj53Ry/JzO3crdnTXb+nnguW2s3LCTdTv6Wb99gKWrt7GxZ/1I4qnUmkszsyPPzPYmprXl6GqJEs+M9iamtuSY1REloSmtOdo0A4FIQ9O/cNkrM2Pe1Ja93kfj7mzvG+aF7gG29Q2xqWeQDd0DrNs+wKadg2zqGeSJF3rY0TfMlt6hvZ6/OZtmenuOzuYs09uamN7WRFdzlqltOdrzWaa3Rtu6WnJ0tmSZ2pIjn01pmLXIJKHkIvvMzJjSmmNKa27cfQvFElt6h9jaO8SG7gE2dA+wtXeYrb2DbOwZpLt/mM07h3h8fTfd/QX6h4ujnqspk6KjOUtnc5b2fIaOfJaO5ixtTRk68hmasmlac2k6mrO05NJ0teRozqbpDOutYb+M+o9EYqfkIrHKpFPM6sgzqyPPi2d3jLt/31CBnoECW3YOsb1/iB19w2zvH2Z7X5SQdg4W2dY7RO9Qga29Qzy3tY/u/mF2DhYYLJSqiimXTtHZkqU1JJzWpgxNmRRNmTQdzRmaMlGSastnyGVS5DNRwsplUjRlUnSG5Vw6RXs+Q3MuTVMmTVtThrTuKRIBlFxkgmnJZWjJZZi1H3OouTu9Q0V2DhTYOTjMjv4C/UNFtvcP0T9UpGegwM7BAr1DBbb3DjNQKLKjf5j+oSI7Bwts6hmkZ32BwUK0PjBcXbIqM4sSV1MmRVtThpaQtNqaMrTk0mTTKfLZNO35DNl0KkpcobaVy0Tr7fksubSRSaVIp20kgVXu35JNk82kyKSMbDqlhCYTkpKLNAwzo60pEwYLHPgEn6WSM1QsjSSfwUKR/qESPQPDDBZLDBVK7OgfZrBQYnC4SHd5uVCie2CYweESgyGBbd45xFChRP9wkb6hAkOFEsNFZ6BQ5EBvNWsKNapsOkUmHSWctqbMSOLJhmTVns+QCcuZlJFJG21NWbJpI50yMikjnUqRzRgt2TTpdIq0ReWpsD2qpaXIpFKkUkRJMGW0NqVHltNmpFKMLDdl0uRz0bnSKVO/2UFCyUVkFKmUkU+lyWfTVfUv7Y9CscRAocRwSEo9A8MMF51iyRkulRgcLtE3VGC4GG3vHyrSN1SkUIqSU6Ho7ByMjhkuligUncFCkd6hIoViiUIp2meoWOK5rX0USuHcxRLDxRI7BwoUvVyWzA3VZoQEFNXMmnPpaN2in3k6ZaTCejpltOQyUYKrSFrR9mjf5lyaXDqFGVFZ2M/Ky+G8rbkMKQvlKRtZLl8rZRbVPkNT6Mj5UjayHK3vOq45myaTtpFjRztn+diWXJpUyjAY2YeK7eVywrlSZiO11slGyUWkjjLpFG3pFDRF64d01veRCqWSR0lsOEpgpRK7vfcNFRkuliiWfLdEtXOwQLHklNwplqLzlJPWwHCRwUJppKxUPtZ9pHYXHbvruPJy+Zrlc5dKjFy/5NGrb7A4ci13QgzROdxDHEWnd6hAaZJOSLIraYERJSzbIylZ5T6VyWyPJLfrfVcCLB9TS0ouIjIiFWoCzbnGnQbIy8krJKc9E9JQocRQMUqGpYp9dzuuFL33DkY1Px85X/TOHuvuUULtGyqObHN27cNu++56d5yB4RKFYilKluz9Gl5xnfJ5C8UosUfHVXyG0q7r7/6zqO3PWclFRA4qUXMZpFHfz56+d0ntztWQE1eaWQ+wot5xVGE6sLneQVRBcdaW4qydyRAjTJ44j3b39lqcqFFrLitqNbNnnMxsqeKsHcVZW5MhzskQI0yuOGt1rsk3BEFERCY8JRcREam5Rk0uV9U7gCopztpSnLU1GeKcDDHCQRhnQ3boi4hIfTVqzUVEROpIyUVERGqu4ZKLmZ1tZivMbJWZXV6H619tZhvN7NGKsqlmtsTMVob3KaHczOyrIdZlZrao4piLw/4rzeziGsc4z8xuN7PHzGy5mX1ggsaZN7N7zezhEOenQvkCM7snxPMDM8uF8qawvipsn19xritC+QozO6uWcVZcI21mD5rZTRM1TjNbbWaPmNlD5WGnE+33Hs7fZWbXm9kTZva4mZ0y0eI0s6PDz7H86jazD07AOP8q/Pt51My+H/5dxf/d9PKUAQ3wAtLAU8CLgBzwMHBswjG8BlgEPFpR9o/A5WH5cuALYfmNwH8DBpwM3BPKpwJPh/cpYXlKDWOcDSwKy+3Ak8CxEzBOA9rCcha4J1z/OuDCUP4N4M/D8vuAb4TlC4EfhOVjw3ehCVgQviPpGH73HwL+C7gprE+4OIHVwPQ9yibU7z1c4xrgf4flHNA1EeOsiDcNvAAcPpHiBOYAzwDNFd/J9yTx3az5D7meL+AU4JaK9SuAK+oQx3x2Ty4rgNlheTbRTZ4A3wTeued+wDuBb1aU77ZfDPHeAJwxkeMEWoAHgJOI7nTO7Pk7B24BTgnLmbCf7fk9qNyvhvHNBW4FTgduCtediHGu5g+Ty4T6vQOdRP8h2kSOc4/YzgR+N9HiJEouzxMlrkz4bp6VxHez0ZrFyj/IsjWhrN5mufv6sPwCMCssjxZvYp8jVHtfTlQrmHBxhqamh4CNwBKiv5i2u3thL9cciSds3wFMSyJO4CvA3wDlJ4xNm6BxOvBLM7vfzC4NZRPt974A2AT8v9DM+G0za52AcVa6EPh+WJ4wcbr7WuCfgeeA9UTftftJ4LvZaMllwvMo7U+I8d9m1gb8CPigu3dXbpsocbp70d1PIKoZnAgcU9+I/pCZvRnY6O731zuWKpzq7ouAc4DLzOw1lRsnyO89Q9S0/HV3fznQS9S8NGKCxAlA6K84F/jhntvqHWfo7zmPKGEfCrQCZydx7UZLLmuBeRXrc0NZvW0ws9kA4X1jKB8t3tg/h5lliRLLf7r7jydqnGXuvh24nagK32Vm5XnxKq85Ek/Y3glsSSDOVwPnmtlq4FqiprF/mYBxlv+Sxd03Aj8hStgT7fe+Bljj7veE9euJks1Ei7PsHOABd98Q1idSnG8AnnH3Te4+DPyY6Psa+3ez0ZLLfcDCMBIiR1RVvbHOMUEUQ3kEyMVEfRzl8ovCKJKTgR2hOn0LcKaZTQl/eZwZymrCzAz4DvC4u39pAsc5w8y6wnIzUb/Q40RJ5oJR4izHfwFwW/jL8UbgwjASZgGwELi3VnG6+xXuPtfd5xN9525z93dNtDjNrNXM2svLRL+vR5lgv3d3fwF43syODkWvBx6baHFWeCe7msTK8UyUOJ8DTjazlvDvvvyzjP+7GUfnVj1fRCMyniRqm/+7Olz/+0Rtm8NEf4FdQtRmeSuwEvgVMDXsa8D/DbE+AiyuOM//AlaF13trHOOpRFX1ZcBD4fXGCRjnS4EHQ5yPAp8I5S8KX+xVRE0RTaE8H9ZXhe0vqjjX34X4VwDnxPj7P41do8UmVJwhnofDa3n538dE+72H858ALA2/+58SjaKaiHG2Ev1l31lRNqHiBD4FPBH+DX2PaMRX7N9NTf8iIiI112jNYiIiMgEouYiISM0puYiISM0puYiISM0puYiISM0puUjDMbOi7T5b7fz9OMf5ZnZsDOFhZvPNzM3s/RVlXzOz99To/HeY2eJanEtkfym5SCPqd/cTKl6r9+Mc5xPNBFu1ijueq7ER+EB5qvOJYh8/g8iolFzkoGBmrzCzO8OEjbdUTM/xp2Z2n0XPjPlRuJP5VURzRf1TqPkcUVkbMLPpYaoXzOw9Znajmd0G3Brugr/aoufQPGhm540S0iaiG+0u3nPDONf6qUXPCFltZn9hZh8K17nbzKZWnObdIfZHzezEcPxeY9vzMxzoz1oElFykMTVXNIn9xKJ51P4VuMDdXwFcDXwu7Ptjd3+lu7+MaGqZS9z990TTXXwk1HyeGud6i8K5X0t0F/Nt7n4i8DqiBNU6ynFfAP7azNL78NmOB94KvDJ8hj6PJne8C7ioYr8Wjyb8fF/4vIwTW+VnEDlgqgJLI+oP/7ECYGbHE/2nvCSaXok00RQ9AMeb2WeJHkbVxv7N6bTE3beG5TOJJrH867CeBw4jSly7cfenzewe4E/24Vq3u3sP0GNmO4CfhfJHiKbLKft+uMavzawjzNE2Wmx7fgaRA6bkIgcDA5a7+yl72fbvwPnu/nDoUD9tlHMU2FXTz++xrXePa73N3VdUGdvniWb9vbPKaw1WLJcq1kvs/u95z3mdfLTYzOykPT6DyAFTs5gcDFYAM8zsFIgeN2Bmx4Vt7cD60HT2ropjesK2stXAK8LyBYzuFuD9YQZazOzlYwXm7k8QzVL7lv241lj+OFz/VKLZd3fsa2wiB0LJRRqeuw8R/Sf9BTN7mGgW6FeFzR8negrn74hmji27FvhI6Pg+guhpfn9uZg8C08e43GeALLDMzJaH9fF8juj5GGXVXmssA+H4bxDNzL2/sYnsF82KLCIiNaeai4iI1JySi4iI1JySi4iI1JySi4iI1JySi4iI1JySi4iI1JySi4iI1Nz/B1EMzy1CA2NvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plotting feature vs frequency graph\n",
    "\n",
    "plot_x = np.array([i+1 for i in range(len(sorted_word_dict))])\n",
    "plot_y = value_arr\n",
    "\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.axis([0,8000,1,5000])\n",
    "plt.xlabel(\"Feature Number\")\n",
    "plt.ylabel(\"Number of Occurences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.1876893043518\n"
     ]
    }
   ],
   "source": [
    "## preparing datasets for x_train of the model\n",
    "\n",
    "## this block takes aroudn 144 seconds to execute since the data has 2000 features, 15000 rows, \n",
    "## and each row is further tokenized into separate words\n",
    "a = time.time()\n",
    "\n",
    "## I am choosing the first 2000 features as beyond that there is no significant change in the accuracy\n",
    "## of the model (found by trial-and-error)\n",
    "\n",
    "num_features = 2000\n",
    "\n",
    "## reducing vocabulary and other parameters to the first 2000 features\n",
    "sorted_word_dict = sorted_word_dict[:num_features]\n",
    "vocab = vocab[:num_features]\n",
    "value_arr = value_arr[:num_features]\n",
    "\n",
    "## initializing numpy arrays for the model which will eventually be passed\n",
    "## these are 2D arrays having data-points as rows and each element of vocab array as a feature\n",
    "## so we have 2000 features for this model\n",
    "x_train_for_model = np.zeros((len(x_train), num_features))\n",
    "x_test_for_model = np.zeros((len(x_test), num_features))\n",
    "\n",
    "## populating x_train_for_model\n",
    "for i in range(len(x_train)):\n",
    "    \n",
    "    ## tokenizing current word using regex\n",
    "    words = re.findall(\"[a-z]+\", x_train[i][1])\n",
    "    \n",
    "    ## iterating over separate words and adding them iff they belong to the vocab\n",
    "    for word in words:\n",
    "        \n",
    "        if word in vocab:\n",
    "            x_train_for_model[i][vocab.index(word)]+=1\n",
    "    \n",
    "print(\"The block executed in:\",time.time()-a,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The block executed in: 29.92203712463379 seconds\n"
     ]
    }
   ],
   "source": [
    "## preparing datasets of the x_test of the model, proceeds exactly like previous block\n",
    "## note that vocab was created using x_train only but now we process x_test according to that vocab only\n",
    "\n",
    "## this block executes in around 30 seconds\n",
    "\n",
    "a=time.time()\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    \n",
    "    ## iterating over separate words and adding them iff they belong to the vocab\n",
    "    \n",
    "    words = re.findall(\"[a-z]+\", x_test[i][1])\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if word in vocab:\n",
    "            \n",
    "            x_test_for_model[i][vocab.index(word)]+=1\n",
    "    \n",
    "print(\"The block executed in:\",time.time()-a,\"seconds\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-2: Model using sklearn\n",
    "\n",
    "Here we train the data on in-built multinomial naive bayes module and print its confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score: 0.8942455157698206\n",
      "testing score: 0.8634\n"
     ]
    }
   ],
   "source": [
    "clf1 = MultinomialNB()\n",
    "clf1.fit(x_train_for_model, y_train)\n",
    "y_pred = clf1.predict(x_test_for_model)\n",
    "\n",
    "print(\"training score:\",clf.score(x_train_for_model, y_train))\n",
    "print(\"testing score:\",clf.score(x_test_for_model, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0  58]\n",
      " [  1 186   5   1   3  20   1   0   0   0   0   3   1   2   5   0   0   0\n",
      "    1   1]\n",
      " [  0   6 188   7   1  22   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  13  18 201  20   5   3   0   0   0   0   0   4   1   0   0   1   0\n",
      "    0   0]\n",
      " [  1  11   3  29 254   8   2   1   0   0   0   0   5   3   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4  16   2   0 167   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   7   7   4   5   3 236   6   2   2   1   2   1   4   0   0   0   3\n",
      "    1   1]\n",
      " [  3   2   3   0   0   0   3 243   5   1   0   0   5   1   1   0   0   1\n",
      "    1   0]\n",
      " [  1   1   0   1   0   1   0  12 226   2   0   0   1   3   1   0   1   1\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   2   2 224  10   0   0   1   1   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  19 229   0   0   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   1   0   0   0   0   0 241   0   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   3   2   3   1   4   6   5   1   0   0   1 219   3   8   0   0   0\n",
      "    1   0]\n",
      " [  3   3   1   0   0   0   0   0   0   0   0   1   3 212   1   0   0   0\n",
      "    0   0]\n",
      " [  2   0   0   0   0   2   1   0   0   1   0   1   0   0 239   0   0   0\n",
      "    4   2]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 281   0   0\n",
      "    1   8]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   2   0   0   0   0 232   5\n",
      "   31  10]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0 234\n",
      "   11   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   1   0  13  17\n",
      "  154  17]\n",
      " [ 33   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   7   0\n",
      "   25 151]]\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.77      0.81      0.79       246\n",
      "           comp.graphics       0.81      0.79      0.80       236\n",
      " comp.os.ms-windows.misc       0.84      0.77      0.80       244\n",
      "comp.sys.ibm.pc.hardware       0.76      0.81      0.78       248\n",
      "   comp.sys.mac.hardware       0.80      0.89      0.85       284\n",
      "          comp.windows.x       0.88      0.72      0.79       233\n",
      "            misc.forsale       0.83      0.93      0.88       253\n",
      "               rec.autos       0.90      0.90      0.90       269\n",
      "         rec.motorcycles       0.90      0.96      0.93       236\n",
      "      rec.sport.baseball       0.93      0.90      0.91       249\n",
      "        rec.sport.hockey       0.92      0.95      0.93       240\n",
      "               sci.crypt       0.98      0.95      0.97       253\n",
      "         sci.electronics       0.85      0.91      0.88       240\n",
      "                 sci.med       0.95      0.91      0.93       233\n",
      "               sci.space       0.95      0.92      0.94       259\n",
      "  soc.religion.christian       0.97      1.00      0.98       281\n",
      "      talk.politics.guns       0.83      0.91      0.86       256\n",
      "   talk.politics.mideast       0.95      0.90      0.92       261\n",
      "      talk.politics.misc       0.76      0.67      0.71       231\n",
      "      talk.religion.misc       0.70      0.61      0.65       248\n",
      "\n",
      "                accuracy                           0.86      5000\n",
      "               macro avg       0.86      0.86      0.86      5000\n",
      "            weighted avg       0.86      0.86      0.86      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## analysis of model via various metrics\n",
    "\n",
    "confusion_matrix_ = confusion_matrix(y_pred, y_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(confusion_matrix_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that our model has 0.86 accuracy on test data. The confusion matrix above also indicates that most of the data points are being classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage-3: Model using self-implemented algorithm\n",
    "\n",
    "Here we train the data on self implemented multinomial naive bayes and print its confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class implementing multinomial_naive_bayes\n",
    "\n",
    "class Multinomial_Naive_Bayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## the main dictionary is a data member of our class\n",
    "        self.result = {}\n",
    "    \n",
    "    ## this is exactly like what was done in the coding session by Ankush sir, so I believe this does not \n",
    "    ## require much explanation\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        class_values = set(y)\n",
    "        y=np.array(y)\n",
    "        \n",
    "        ## stores the total number of data points\n",
    "        self.result[\"total_data\"] = len(y)\n",
    "        \n",
    "        for current_class in class_values:\n",
    "            \n",
    "            self.result[current_class] = {}\n",
    "            \n",
    "            x_current = x[y==current_class]\n",
    "            y_current = y[y==current_class]\n",
    "            \n",
    "            num_features = x.shape[1]\n",
    "            \n",
    "            ## stores the total number of data_points in current_class \n",
    "            self.result[current_class][\"total_count\"] = len(y_current)\n",
    "            \n",
    "            ## stores total word count for current_class\n",
    "            temp=0\n",
    "            for j in range(num_features):\n",
    "            \n",
    "                temp+=x_current[:,j].sum()\n",
    "                self.result[current_class][j] = x_current[:,j].sum()\n",
    "                \n",
    "            ## stores total word count for current_class\n",
    "            self.result[current_class][\"word_in_class\"]=temp\n",
    "                \n",
    "                \n",
    "    def probability(self, data_point, current_class):\n",
    "        \n",
    "        ## this is for weighted addition of log probabilties \n",
    "        ## laplace correction is also implemented\n",
    "        \n",
    "        output = np.log(self.result[current_class][\"total_count\"])-np.log(self.result[\"total_data\"])\n",
    "        \n",
    "        ## subtract 2 because of total_count and word_in_class\n",
    "        num_features = len(self.result[current_class].keys())-2\n",
    "        \n",
    "        for j in range(num_features):\n",
    "            \n",
    "            ## value of feature j for current_class\n",
    "            xj = data_point[j]\n",
    "            \n",
    "            if xj==0:\n",
    "                continue\n",
    "                \n",
    "            ## numerator\n",
    "            count_current_class_with_value_xj = self.result[current_class][j]+1\n",
    "            \n",
    "            ## denominator\n",
    "            laplace = len(self.result[current_class].keys())-2\n",
    "            count_current_class = self.result[current_class][\"word_in_class\"] + laplace\n",
    "            \n",
    "            current_xj_probability = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "        \n",
    "            output = output + current_xj_probability\n",
    "            \n",
    "        return output\n",
    "        \n",
    "        \n",
    "    def predict_single_point(self, data_point):\n",
    "        \n",
    "        all_classes = self.result.keys()\n",
    "        \n",
    "        best_p = -float(\"inf\")\n",
    "        best_class = -1\n",
    "        first_run = True\n",
    "        \n",
    "        for current_class in all_classes:\n",
    "            \n",
    "            if current_class == \"total_data\":\n",
    "                continue\n",
    "                \n",
    "            p_current_class = self.probability(data_point, current_class)\n",
    "            \n",
    "            if first_run or p_current_class > best_p:\n",
    "                best_p = p_current_class\n",
    "                best_class = current_class\n",
    "                \n",
    "            first_run = False\n",
    "            \n",
    "        #print(\"iteration number:\", self.counter,\"took:\", time.time()-a,\"seconds\")\n",
    "        #self.counter+=1\n",
    "        return best_class\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        y_pred = []\n",
    "        \n",
    "        for data_point in x:\n",
    "            \n",
    "            data_point_class = self.predict_single_point(data_point)\n",
    "            y_pred.append(data_point_class)\n",
    "            \n",
    "        return y_pred\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to find mean accuracy. This is written outside the class so that we can use it without \n",
    "## creating an object as well, for more efficient useage.\n",
    "\n",
    "def score(y_pred, y_test):\n",
    "    count=0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_test[i]:\n",
    "            count+=1\n",
    "    return count/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using self implemented model this time\n",
    "\n",
    "clf2 = Multinomial_Naive_Bayes()\n",
    "clf2.fit(x_train_for_model, y_train)\n",
    "\n",
    "## uncomment to print main algo dictionary\n",
    "##print(clf2.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score: 0.907381476295259\n",
      "testing score: 0.8714\n",
      "The block executed in: 358.9994761943817 seconds\n"
     ]
    }
   ],
   "source": [
    "## call prediction function. This function takes around 140 seconds as well (if we train only test data)\n",
    "## training train data will take around 360 seconds or 6 minutes in my system.\n",
    "\n",
    "a = time.time()\n",
    "\n",
    "y_train_pred = clf2.predict(x_train_for_model)\n",
    "y_test_pred = clf2.predict(x_test_for_model)\n",
    "\n",
    "print(\"training score:\",score(y_train_pred, y_train))\n",
    "print(\"testing score:\",score(y_test_pred, y_test))\n",
    "print(\"The block executed in:\",time.time()-a,\"seconds\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  78]\n",
      " [  1 195  20   4   7  32   0   0   0   0   0   3   3   4   8   0   0   0\n",
      "    1   2]\n",
      " [  0   3 172   2   0  17   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   8  21 209   8   2   3   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   9   5  28 260   6   4   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  10  18   1   2 175   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   5   3   5   0 228   7   3   2   2   1   0   1   1   0   0   2\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   7 249   5   0   0   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  1   0   0   0   0   0   0   7 228   1   1   0   0   1   1   0   1   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   0   0   0   0 240   4   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6 233   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   2   0   0   0   0   0   0   0   0 241   0   0   0   0   2   0\n",
      "    1   0]\n",
      " [  0   5   0   1   2   1   9   5   0   0   0   5 230   3  12   0   0   0\n",
      "    0   2]\n",
      " [  4   4   0   0   0   0   1   0   0   0   0   0   2 224   2   0   0   0\n",
      "    1   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0 232   0   0   0\n",
      "    1   1]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 281   0   0\n",
      "    0   5]\n",
      " [  0   0   0   0   0   0   1   1   0   0   0   1   0   0   0   0 230   5\n",
      "   36  11]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 236\n",
      "    9   1]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   1   0   0   3   0  18  17\n",
      "  159  26]\n",
      " [ 23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5   1\n",
      "   22 122]]\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.73      0.87      0.79       246\n",
      "           comp.graphics       0.70      0.83      0.76       236\n",
      " comp.os.ms-windows.misc       0.89      0.70      0.79       244\n",
      "comp.sys.ibm.pc.hardware       0.83      0.84      0.83       248\n",
      "   comp.sys.mac.hardware       0.83      0.92      0.87       284\n",
      "          comp.windows.x       0.85      0.75      0.80       233\n",
      "            misc.forsale       0.87      0.90      0.89       253\n",
      "               rec.autos       0.95      0.93      0.94       269\n",
      "         rec.motorcycles       0.95      0.97      0.96       236\n",
      "      rec.sport.baseball       0.98      0.96      0.97       249\n",
      "        rec.sport.hockey       0.97      0.97      0.97       240\n",
      "               sci.crypt       0.98      0.95      0.97       253\n",
      "         sci.electronics       0.84      0.96      0.89       240\n",
      "                 sci.med       0.94      0.96      0.95       233\n",
      "               sci.space       0.99      0.90      0.94       259\n",
      "  soc.religion.christian       0.98      1.00      0.99       281\n",
      "      talk.politics.guns       0.81      0.90      0.85       256\n",
      "   talk.politics.mideast       0.96      0.90      0.93       261\n",
      "      talk.politics.misc       0.71      0.69      0.70       231\n",
      "      talk.religion.misc       0.71      0.49      0.58       248\n",
      "\n",
      "                accuracy                           0.87      5000\n",
      "               macro avg       0.87      0.87      0.87      5000\n",
      "            weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## analysis of model via various metrics\n",
    "\n",
    "confusion_matrix_ = confusion_matrix(y_pred, y_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(confusion_matrix_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that our model has 0.87 accuracy on test data. The confusion matrix above also indicates that most of the data points are being classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Final Remarks\n",
    "\n",
    "From the above analysis, the in-built model has an accuracy of 0.86 on the testing data whereas the accuracy increases to 0.87 when the self-implemented model is used. I believe this can be improved more if a more detailed list of stop-words is used. Based on these inferences, the self-implemented model works pretty well in comparison to the inbuilt one. For further comparison, confusion matrices and classification reports in each of the cases can be referred. One more noteworthy point is that the inbuilt algo takes comparatively much lesser time for predictions whereas self implemented model takes aroudn 6 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
