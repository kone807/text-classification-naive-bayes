{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.52      0.65        25\n",
      "           1       0.78      0.48      0.60        29\n",
      "           2       0.77      0.34      0.48        29\n",
      "           3       0.57      0.27      0.36        30\n",
      "           4       0.79      0.52      0.62        29\n",
      "           5       0.55      0.82      0.66        34\n",
      "           6       0.83      0.17      0.29        29\n",
      "           7       0.77      0.57      0.65        30\n",
      "           8       0.95      0.64      0.76        33\n",
      "           9       1.00      0.71      0.83        24\n",
      "          10       0.61      0.83      0.70        24\n",
      "          11       0.49      0.87      0.63        31\n",
      "          12       0.81      0.33      0.47        39\n",
      "          13       0.70      0.88      0.78        26\n",
      "          14       0.58      0.88      0.70        33\n",
      "          15       0.56      0.71      0.63        28\n",
      "          16       0.60      0.91      0.72        23\n",
      "          17       0.38      0.97      0.54        29\n",
      "          18       0.53      0.67      0.59        24\n",
      "          19       0.71      0.29      0.42        17\n",
      "\n",
      "    accuracy                           0.62       566\n",
      "   macro avg       0.69      0.62      0.60       566\n",
      "weighted avg       0.69      0.62      0.60       566\n",
      "\n",
      "[[13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  7  0  0]\n",
      " [ 0 14  2  1  0  3  0  0  0  0  0  5  0  1  2  0  0  1  0  0]\n",
      " [ 0  1 10  2  0  8  0  0  0  0  1  2  0  2  1  0  0  2  0  0]\n",
      " [ 0  1  1  8  1  3  1  0  1  0  0  3  2  1  3  0  1  2  0  2]\n",
      " [ 0  1  0  1 15  4  0  0  0  0  0  1  0  2  3  1  0  1  0  0]\n",
      " [ 0  0  0  0  0 28  0  0  0  0  1  3  0  1  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  3  1  5  3  0  0  3  2  1  0  4  1  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0  0  1  0  1  0  0  3  4  4  0]\n",
      " [ 0  0  0  0  0  0  0  1 21  0  2  0  0  1  0  0  2  5  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 17  5  0  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 20  0  0  0  1  0  1  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 27  0  0  0  1  1  1  1  0]\n",
      " [ 0  1  0  2  0  3  0  1  0  0  0 10 13  0  6  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 23  0  0  1  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0 29  0  1  1  1  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  1  1 20  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1 21  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28  1  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  2  2  1  5]]\n"
     ]
    }
   ],
   "source": [
    "# our own implementation of naive bayes \n",
    "\n",
    "#import all the required libraries that will be needed throughout the code \n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import string\n",
    "import csv \n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# a program to fetch all the commonwords from the csv file and store them in a list and then return the list to us.The csv of the common word is attached. Also we have converted all the words to lower case so that case sensitivity is taken care of    \n",
    "def commonwords():\n",
    "    li=list()\n",
    "    with open(\"Commonwords.csv\",'r') as obj1:\n",
    "        obj_data=csv.reader(obj1)\n",
    "        for i in obj_data:\n",
    "            if len(i)!=0:\n",
    "                k=i[0].split()\n",
    "                for z in k:\n",
    "                    li.append(str.lower(z))\n",
    "\n",
    "    return li\n",
    "\n",
    "#the trainer program accepts the input sentences and then converts them into words and cound the occourance of each word in each class and then returns a final dictionary that consists of the  target values as level 1 and then in each class we have the total of frequency of each word as a count \n",
    "def trainer(x_train,y_train):\n",
    "    master={}\n",
    "    #we make a master dictionary and then call the commonwords function to get the list of commonwords with us \n",
    "    li=commonwords()\n",
    "    #now we will iterate over each and every training point \n",
    "    for i in range(len(x_train)):\n",
    "        master[y_train[i]]=master.get(y_train[i],{})\n",
    "        #now we will first look for its class and in taking that class of the dictionary as the key we will iterate and first of all we will convert all the sentences into words and remove out all the punctuations from them \n",
    "        j=(x_train[i].translate(str.maketrans('','', string.punctuation)).split())\n",
    "        #now for each and every word we will check that if that word is not a part of common words we will then add it for that particular class and increase the frequency accordingly \n",
    "        for k in j:\n",
    "            k=str.lower(k)\n",
    "            if k not in li:\n",
    "                master[y_train[i]][k]=master[y_train[i]].get(k,0)+1\n",
    "    master2={}\n",
    "    #now we will filter out the words based on a certain lower cut which means that words below that frequency would be ignored \n",
    "    for i in master:\n",
    "        master2[i]=master2.get(i,{})\n",
    "        for j in master[i]:\n",
    "            if master[i][j]>5:\n",
    "                master2[i][j]=master2[i].get(j,master[i][j])\n",
    "    \n",
    "    #now we will return the filtered dictionary and the commonwords list along with that as well \n",
    "    return master2,li\n",
    "\n",
    "#now we will use this function to find out the total number of words that we have in our training data after filtering and store that in the dictionary\n",
    "def total_adder(master2):\n",
    "    for i in master2:\n",
    "        sum=0\n",
    "        for j in master2[i]:\n",
    "            sum+=master2[i][j]\n",
    "        master2[i][\"total\"]=sum\n",
    "    return master2\n",
    "\n",
    "#now we will pass the test value data to another function along with that will filter them out in the same way as we did for the training data i.e. by making the dicitonary of each test value with important words and their frequency and will remove the punctuations and then we will append those dictionaries into the new list and return the list  \n",
    "def test_value_filter(x_test,li):\n",
    "    test_val=list()\n",
    "    for i in range(len(x_test)):\n",
    "        d=dict()\n",
    "        j=(x_test[i].translate(str.maketrans('','', string.punctuation)).split())\n",
    "        for k in j:\n",
    "            k=str.lower(k)\n",
    "            if k not in li:\n",
    "                d[k]=d.get(k,0)+1\n",
    "        test_val.append(d)\n",
    "    return test_val\n",
    "\n",
    "# it is used to find the probablity of each class and then store it in a dictionary and return it \n",
    "def prob_finder(y_train):\n",
    "    y1=dict()\n",
    "    for i in y_train:\n",
    "        y1[i]=y1.get(i,0)+1\n",
    "    t=len(y_train)\n",
    "    y_prob=dict()\n",
    "    for i in y1:\n",
    "        y_prob[i]=math.log(y1[i]/t)\n",
    "    return y_prob\n",
    "\n",
    "#the main  predicter function is used to find out and predict the classes from it thus we will pass and iterate over the each value and then compare it with each class of the list and we will allot the class with the highest probabllity and predict that for the particular iteration of that test case  \n",
    "def predicter(master2,test_val,y_train):\n",
    "    my_pred=list()\n",
    "    #making call to know class wise probablity and get that in a dictionary \n",
    "    y_prob=prob_finder(y_train)\n",
    "    \n",
    "    for i in test_val:\n",
    "        #for each iteratrion in test_val we will check on every possible classes and then will update them with the maximum one here we will be working on logarthmic probablities.Also since we will be skipping the 0 cases thus there will not be the need of La place correction as 0 probablity values are simply skipped \n",
    "        yp=-1\n",
    "        prob=0\n",
    "        for j in master2:\n",
    "            prob2=0\n",
    "            for k in i:\n",
    "                if k in master2[j]:\n",
    "                    prob2+=i[k]*(math.log(master2[j][k]/master2[j][\"total\"]))\n",
    "            prob2+=y_prob[j]\n",
    "            if prob2<=prob:\n",
    "                prob=prob2\n",
    "                yp=j\n",
    "        #now after making a decision we will append them in an array and return that array \n",
    "        my_pred.append(yp)\n",
    "    \n",
    "    return my_pred\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "# we will make call to the 20newsgroup dataset in sklearn and remove the header footer  from it \n",
    "news = datasets.fetch_20newsgroups(remove=(\"headers\",\"footers\")) \n",
    "x = news.data\n",
    "y=news.target\n",
    "#now we will split the dataset into training and testing data \n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.05, random_state = 5)\n",
    "# we will make the call to the training fuction to get the list of commonwords and to get our trained data dictionary \n",
    "master2,li=trainer(x_train,y_train)\n",
    "#now we will make call to total adder to add the total values \n",
    "master2=total_adder(master2)\n",
    "#nwe make a call and filter the text value as the same way as we do for training dat \n",
    "test_val=test_value_filter(x_test,li)\n",
    "#now we will make a call to predict function to make the predictions and get the results \n",
    "my_pred=predicter(master2,test_val,y_train)\n",
    "#now we will print its classification report and confusion matrix comparing it with the actual values \n",
    "print(classification_report(y_test,my_pred))\n",
    "print(confusion_matrix(y_test,my_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using  inbuilt multinomial NB \n",
    "\n",
    "#import all the required libraries that will be needed throughout the code \n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import string\n",
    "import csv \n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# a program to fetch all the commonwords from the csv file and store them in a list and then return the list to us.The csv of the common word is attached. Also we have converted all the words to lower case so that case sensitivity is taken care of    \n",
    "\n",
    "def getcommonwords():\n",
    "    li=list()\n",
    "    with open(\"Commonwords.csv\",'r') as obj1:\n",
    "        obj_data=csv.reader(obj1)\n",
    "        for i in obj_data:\n",
    "            if len(i)!=0:\n",
    "                k=i[0].split()\n",
    "                for z in k:\n",
    "                    li.append(str.lower(z))\n",
    "    return li\n",
    "# This fuction is used to make a dictionary of all the words present in training data by filtering out the common wordss and punctuation marks and then return the dictionary \n",
    "def get_all_training_words(li):\n",
    "    df=dict()\n",
    "    for i in x_train:\n",
    "        j=(i.translate(str.maketrans('','', string.punctuation)).split())\n",
    "        for k in j:\n",
    "            k=str.lower(k)\n",
    "            if k not in li:\n",
    "                if k not in df:\n",
    "                    df[k]=0\n",
    "    return df\n",
    "#this fucntion iterates obver the training data and then predicts the frequency of each and every word and its number of occourances class wisee thus at primary level we have the classes as keys and at secondary level we have the words and their frequency in that class \n",
    "def get_master(df,li,x_train,y_train):\n",
    "    master={}\n",
    "    #we make a master dictionary and then call the commonwords function to get the list of commonwords with us \n",
    "\n",
    "    #now we will iterate over each and every training point \n",
    "    for i in range(len(x_train)):\n",
    "        master[y_train[i]]=master.get(y_train[i],{})\n",
    "        #now we will first look for its class and in taking that class of the dictionary as the key we will iterate and first of all we will convert all the sentences into words and remove out all the punctuations from them \n",
    "        j=(x_train[i].translate(str.maketrans('','', string.punctuation)).split())\n",
    "        #now for each and every word we will check that if that word is not a part of common words we will then add it for that particular class and increase the frequency accordingly \n",
    "        for k in j:\n",
    "            k=str.lower(k)\n",
    "            if k not in li:\n",
    "                master[y_train[i]][k]=master[y_train[i]].get(k,0)+1\n",
    "    master2={}\n",
    "    #now we will filter out the words based on a certain lower cut which means that words below that frequency would be ignored \n",
    "    for i in master:\n",
    "        master2[i]=master2.get(i,{})\n",
    "        for j in master[i]:\n",
    "            if master[i][j]>5:\n",
    "                master2[i][j]=master2[i].get(j,master[i][j])\n",
    "                \n",
    "\n",
    "    #now we will return the filtered dictionary and the commonwords list along with that as well \n",
    "    return master2\n",
    "#now the data filter function converts the data in the form so that it could be send to the training algorithm \n",
    "def data_fitter(master2,li,df):\n",
    "    x_li=list()\n",
    "    y_li=list()\n",
    "    #we will iterate over each class in master2 dictionary and then we will check for the word from df dictionary if the word is present in that class we will append its frequency else we will append 0 \n",
    "    for i in master2:\n",
    "        l2=list()\n",
    "        for k in df:\n",
    "                if k in master2[i]:\n",
    "                    l2.append(master2[i][k])\n",
    "                else:\n",
    "                    l2.append(0)\n",
    "        #now we will append the list that contains freq of all words in order and the value of y for it into 2 separate list and return them \n",
    "        x_li.append(l2)\n",
    "        y_li.append(i)\n",
    "    return x_li,y_li\n",
    "\n",
    "#the test converter converts all the testing data into the dictionary with frequency of all the words after filtering out all the punctuations and common words \n",
    "def test_converter(li):\n",
    "    test_val=list()\n",
    "    for i in range(len(x_test)):\n",
    "        d=dict()\n",
    "        j=(x_test[i].translate(str.maketrans('','', string.punctuation)).split())\n",
    "        for k in j:\n",
    "            k=str.lower(k)\n",
    "            if k not in li:\n",
    "                d[k]=d.get(k,0)+1\n",
    "        test_val.append(d)\n",
    "    return test_val\n",
    "# this function is used to convert the testing data in form of the list that contain freq of each word in the order \n",
    "def test_fitter(df,test_val):\n",
    "    x_pred=list()\n",
    "    for i in test_val:\n",
    "        l2=list()\n",
    "        # we iterate over every testing data and then will check if its present in our test data or not and append freq accordingly\n",
    "        for k in df:\n",
    "            if k in i:\n",
    "                l2.append(i[k])\n",
    "            else:\n",
    "                l2.append(0)\n",
    "        x_pred.append(l2)\n",
    "    return x_pred\n",
    "# we will make call to the 20newsgroup dataset in sklearn and remove the header footer  from it    \n",
    "news = datasets.fetch_20newsgroups(remove=(\"headers\",\"footers\")) \n",
    "x = news.data\n",
    "y=news.target\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.05, random_state = 5)\n",
    "#call made to programm to get the list of all common words \n",
    "li=getcommonwords()\n",
    "#to get all the words that we have \n",
    "df=get_all_training_words(li)\n",
    "#to convert all the data and get the dictionary with classes and frequency of all words\n",
    "master2=get_master(df,li,x_train,y_train)\n",
    "#to convert the data into the training format \n",
    "x_li,y_li=data_fitter(master2,li,df)\n",
    "#to convert testing data to fformat for dictionay\n",
    "test_val=test_converter(li)\n",
    "# to convert test data dictionary fit for predictions \n",
    "x_pred=test_fitter(df,test_val)\n",
    "#to make call to multinomialNB and train algo for it \n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_li,y_li)\n",
    "#to get the predictions from inbuilt MUltinomial NB \n",
    "Y_pred = clf.predict(x_pred)\n",
    "#now we will print its classification report and confusion matrix comparing it with the actual values \n",
    "print(classification_report(y_test,Y_pred))\n",
    "print(confusion_matrix(y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.56      0.70        25\n",
      "           1       0.78      0.52      0.62        27\n",
      "           2       0.77      0.53      0.62        19\n",
      "           3       0.86      0.39      0.53        31\n",
      "           4       0.84      0.62      0.71        26\n",
      "           5       0.67      0.72      0.69        47\n",
      "           6       1.00      0.29      0.44        21\n",
      "           7       0.91      0.67      0.77        30\n",
      "           8       0.91      0.74      0.82        27\n",
      "           9       1.00      0.71      0.83        24\n",
      "          10       0.64      0.88      0.74        24\n",
      "          11       0.58      0.78      0.67        41\n",
      "          12       0.81      0.52      0.63        25\n",
      "          13       0.73      0.96      0.83        25\n",
      "          14       0.70      0.95      0.80        37\n",
      "          15       0.78      0.65      0.71        43\n",
      "          16       0.77      0.84      0.81        32\n",
      "          17       0.41      0.91      0.56        33\n",
      "          18       0.57      0.68      0.62        25\n",
      "          19       0.29      0.50      0.36         4\n",
      "\n",
      "    accuracy                           0.69       566\n",
      "   macro avg       0.75      0.67      0.67       566\n",
      "weighted avg       0.75      0.69      0.69       566\n",
      "\n",
      "[[14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  1  5  0  1]\n",
      " [ 0 14  1  1  0  2  0  0  0  0  2  2  0  1  1  0  0  3  0  0]\n",
      " [ 0  1 10  0  0  1  0  0  0  0  0  1  0  2  1  0  0  2  1  0]\n",
      " [ 0  1  2 12  0  7  0  0  0  0  0  3  1  0  1  0  1  2  0  1]\n",
      " [ 0  2  0  0 16  2  0  0  0  0  0  1  0  1  2  1  0  1  0  0]\n",
      " [ 1  0  0  0  0 34  0  0  0  0  1  7  1  1  1  0  0  0  1  0]\n",
      " [ 0  0  0  1  3  1  6  0  1  0  3  0  1  0  1  1  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  1  0  0  1  0  0  2  0  1  2  3  0]\n",
      " [ 0  0  0  0  0  0  0  1 20  0  1  0  0  1  0  0  1  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 17  4  0  0  0  0  0  1  0  2  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 21  0  0  0  1  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0 32  0  1  2  0  0  3  1  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  0  0  6 13  0  1  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 24  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1 35  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  1  1 28  1  8  1  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1 27  2  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0 30  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  6 17  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  2]]\n"
     ]
    }
   ],
   "source": [
    "# now we will compare the values of inbuilt multinomialNB with the values we predicted using our own function and print its classification and confusion matrix \n",
    "print(classification_report(Y_pred,my_pred))\n",
    "print(confusion_matrix(Y_pred,my_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
